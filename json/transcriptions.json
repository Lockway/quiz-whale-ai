[
    {
        "page_number": 1,
        "content": "이번 슬라이드에서는 머신러닝 알고리즘을 크게 세가지 캐테고리로 나누어서 배워보겠습니다. 먼저 이것을 언급하기 전에 AI라고 하는 인공지능에 대해서 정의만 살펴보겠습니다. 여기 인공지능 AI는 문자 그대로 인간이 지능을 가지고 사고하고 학습을 하는데 이런 인간의 지능을 컴퓨터로 구현하는 기술을 말하는 것이고 그리고 이것의 한 분야로써 머신러닝이 있습니다. 이 머신러닝은 컴퓨터 개발자가 구체적으로 컴퓨터에 프로그램을 하지 않은 상태에서 데이터를 기반으로 스스로 학습해서 어떤 패턴, 규칙을 찾아서 성능을 향상시키는 기술을 말하는데요. 대표적으로 여기 밑에 있는 세가지 방식이 있습니다. 그 세가지가 Supervised Learning, Unsupervised Learning, Reinforcement Learning이 있습니다. 먼저는 Supervised Learning, 지도학습인데요. 이것부터 살펴보겠습니다. 이 지도학습은 여기 있는 것처럼 라벨이 있는 데이터를 다룹니다. 즉 정답을 알고 있는 데이터를 다루기 때문에 저희가 지도학습이라고 이렇게 말을 합니다. 그래서 이 머신러닝은 기본적으로 데이터를 토대로 학습을 하잖아요. 그렇기 때문에 많은 양의 데이터를 필요로 하고 있습니다. 그 데이터들을 저희가 여기 트레이닝 데이터라고 하는데, 이 트레이닝 데이터의 특징이 라벨이 있다라는 것입니다. 그 말은 여기 만약에 100만장의 이미지 데이터가 있다고 하겠습니다. 이 이미지들은 각각 동물들 사진인데요. 그 안에 개와 고양이 사진들이 있다고 하겠습니다. 그러면 저희가 하고 싶은 것은 이 100만장의 트레이닝 데이터 이미지를 통해서 이제 전혀 새로운 데이터가 들어왔을 때 그 데이터에 있는 사진이 개인지 고양인지를 말할 수 있는 그런 알고리즘을 만들고 싶습니다. 그런데 이 트레이닝 데이터는 이미 정답을 알고 있죠? 컴퓨터가 즉 뉴럴 네트워크가 학습하는 과정에서 이 트레이닝 데이터에 이미지들이 들어오게 되면 그것을 가지고 나름대로 예측을 해서 아웃풋을 출력을 하는데 그 출력한 아웃풋 값이 실제 정답인 여기 나와있는 라벨하고 비교를 해서 일치하는지 일치하지 않는지를 평가를 해서 반복적으로 학습을 하게 됩니다. 그런 식으로 해서 학습이 충분히 되어지면 이 뉴럴 네트워크는 이미지가 들어왔을 때 그것이 개인지 고양인지를 분별할 수 있는 어떤 특징을 잘 파악하고 있겠죠? 그리고 학습이 잘 맞춰진 후에는 어떻게 될까요? 이제는 라벨이 없는 전혀 새로운 데이터, 여기서 말하는 Remaining 데이터를 말하는데 때로는 저희가 테스트 데이터라고도 말하게 되겠죠? 이 데이터는 아직 라벨이 알려져 있지 않습니다. 그런데 지금까지 학습한 것을 기반으로 해서 이 데이터의 라벨을 예측하는 것을 저희가 목표로 하고 있습니다. 이것이 Supervised Learning이고 이것은 정답을 알고 있기 때문에 비교적 학습이 유리합니다. 이것을 사용하는 대표적인 방법이 Regression과 Classification이 있습니다. 여기 Regression은 회귀분석이라고 하고 Classification은 분류 문제를 다루는 거죠. 그리고 이와 대립되는 것이 Unsupervised Learning입니다. 이것은 비지도학습이라고 하는데요. 비지도학습이라고 하는 이유는 이제는 이 트레이닝 데이터셋을 살펴보면 여기에 라벨이 나와 있지 않습니다. 그러니까 정답을 알려주지 않고 문제를 해결하는 것이죠. 예를 들면 100만 장의 이미지 데이터가 있는데 그것에 라벨이 전혀 없고 그냥 사진만 주어져 있습니다. 이런 경우라면 저희가 새로운 이미지가 들어왔을 때 이것이 개인지 고양인지 말하는 것 자체가 아주 어려운 문제가 되는 거죠. 왜 그러냐면 트레이닝 단계에서 이미 각 이미지마다 이것이 개인지 고양인지를 알려주지 않았잖아요? 그래서 이런 경우에는 클래시피케이션 문제를 풀 수는 없습니다. 그런데 저희가 무엇을 할 수 있냐면 이 이미지들을 보고 비슷한 성질이 있는 그림끼리 서로 묶을 수는 있겠죠. 그래서 그림 안에 있는 그 동물들의 꼬리의 모양, 귀의 모양, 크기 이런 것들을 보면서 비슷한 형태의 그림끼리 분류는 할 수가 있습니다. 그래서 공통적인 특징을 갖는 그런 비슷한 데이터들끼리 그룹핑을 할 수 있습니다. 그것을 군집하라고 하는데 여기에 있는 클러스터딩이 그것을 말합니다. 그래서 비슷한 데이터들끼리 그룹핑을 하는데요. 이때는 중요한 것이 이 이미지들의 패턴을 잘 인식을 해야 되겠죠. 이렇게 하는 방법이 언수퍼바질 러닝인데 수퍼바질 러닝에 비해서 훨씬 학습이 어렵습니다. 그리고 저희가 다루는 대부분의 딥러닝은 수퍼바질 러닝 기법을 사용하고 있습니다. 세번째 방식은 이번 유튜브 영상의 핵심 주제인 리인포스먼 러닝입니다. 강화학습이라고 하는데요. 리인포스먼 러닝을 간단히 RL로 표현할텐데요. 이 강화학습은 역사가 상당히 오래되어 있습니다. 그리고 저희가 나중에 배우게 될 딥 리인포스먼 러닝 이것은 그대로 번역하면 심층 강화학습이 되는데 그런데 요즘은 이것 두가지를 다 강화학습이라고 표현을 합니다. 그런데 지금 설명하려고 하는 이 강화학습은 머신러닝 기법에서의 강화학습으로 이것을 말하고 있습니다. 그런데 이 강화학습은 앞에 있는 수퍼바질 러닝과 언수퍼바질 러닝에 비해서 훨씬 더 어려운 개념인데 오늘은 간단하게 요약만 해보겠습니다. 강화학습의 가장 큰 특징은 여기 샘플 데이터가 없다라는 거에요. 무슨 말이냐면 앞에 수퍼바질 러닝과 언수퍼바질 러닝은 여기 트레이닝 데이터가 있었죠. 이런 데이터를 기반으로 학습하는 것이 딥 러닝이라고 한다면 여기 리인포스먼 러닝은 이런 샘플 데이터가 미리 주어져 있지 않습니다. 이것이 무슨 의미냐면 강화학습은 에이전트가 실제 환경 속에서 상호작용을 하면서 그때그때 얻어지는 그 데이터를 가지고 학습을 한다고 생각하면 됩니다. 그래서 이 강화학습의 주체가 에이전트인데요. 이 에이전트는 강화학습을 통해서 스스로 학습하는 컴퓨터를 지칭합니다. 그러면 이 에이전트가 주어진 환경 속에서 자신의 상태를 인식하는 것을 스테이시라고 합니다. 그래서 자신의 상태를 인식하고 나서 그것에 맞게 어떤 행동을 취하게 되는데 그 행동을 액션이라고 합니다. 그리고 이 액션을 취한 후에 그것의 보상으로 리워드를 받게 되는데 그래서 에이전트는 여기 state, action, reward라는 이 세가지 요소를 가지고 학습을 진행하게 됩니다. 예를 들어서 로봇 워킹에 대해서 생각을 해보겠습니다. 로봇이 걸어가는 것을 트레이닝을 한다고 하겠습니다. 여기에 길이 있고 여기 위를 로봇이 걸어가고 있는데요. 이 로봇이 넘어지지 않고 잘 걸어갈 수 있도록 저희는 학습을 하고자 합니다. 그러면 이 로봇이 걸어갈 때 가장 중요한 부분은 이 로봇의 손과 발에 있는 이 관절의 움직임이겠죠. 그래서 여기 발의 관절과 여기 손의 관절들 이것들을 몇도 각도로 움직여 주냐에 따라서 이 로봇이 걸어갈 수가 있는 거잖아요. 그래서 이 로봇이 걸어갈 수 있도록 이 관절을 움직여 주는 것을 액션이라고 하겠습니다. 행동이라고도 하는데요. 그런데 이 관절을 저희가 조절을 할 때 실제로는 이 로봇의 현재 관절의 상태와 그리고 노면의 상태를 저희가 고려를 해야 되겠죠. 그래서 이 노면이 평평할 때와 여기처럼 장애물이 있을 때 저희가 움직이는 관절 모양이 바뀌게 되는 거잖아요. 그래서 이 로봇의 현재 관절의 상태와 그리고 이 노면의 상태들을 저희가 state이라고 하겠습니다. 그러면 현재 상태가 주어졌을 때 내가 어떤 식으로 액션을 취하는 것이 좋은지 이것을 학습한다고 할 때 만약 이것을 기존의 딥러닝에 있는 슈퍼바디러닝으로 학습한다고 가정을 해 보겠습니다. 그러면 어떤 식으로 해야 되나요? 여기 있는 현재 상태의 state이 input으로 들어오면 이 뉴럴 네트워크의 output으로 내가 어떤 액션을 취해야 될지 정답을 말해 줘야 되는 거죠. 그래서 슈퍼바디러닝으로 풀려면 이 state이 input이 되고 그리고 여기 액션이 정답이기 때문에 라벨이 되는 거네요. 그러면 이것을 학습하기 위해서는 이 state과 액션으로 된 데이터셋이 있어야 되는데 그런데 이 state에 대해서는 여기서 보면 state 종류가 너무도 많죠. 왜 그러냐면 각 관절마다 각도가 여러 가지가 있는데 그 관절들이 하나가 아니라 여러 개가 있는 거죠. 그리고 노면의 상태도 종류가 아주 다양합니다. 이 모든 종류의 state을 다 고려할 수 있는 그 데이터셋을 구하려면 실제로는 너무도 방대해지고 그리고 각 state마다 액션을 어떻게 취할지 그 라벨을 결정하는 것도 너무도 방대한 일이 되죠. 그래서 실제로는 이런 종류의 데이터들은 저희가 구할 수가 없습니다. 그래서 이 경우에는 여기 있는 것처럼 미리 샘플 데이터를 구할 수 있는 방법도 없고 구한다 하더라도 그것을 학습하기에는 너무 방대한 일이 되기 때문에 이런 데이터셋을 구하려면 그것을 학습하기에는 너무 방대한 일이 되기 때문에 기존의 슈퍼바디러닝을 사용한 이 방법으로는 거의 해결할 수가 없습니다. 그래서 강화학습에서는 처음부터 이 액션에 대한 정답을 줄 수가 없기 때문에 실제 환경에서 적절하게 액션을 취해보고 그 후의 반응을 보고 이 액션을 조금씩 수정하는 방향으로 그러니까 이것은 시간에 의해서 계속 변하는 거죠. 이런 식으로 이 액션을 임프로브하는 방식으로 학습을 합니다. 그런데 이 액션을 개선하는 즉 임프로브하는 그 기준은 뭐냐면 여러 가지 스테이드에서 액션들을 취해봤는데 이 로봇이 자주 넘어지면 이건 좋은 액션이 아니죠. 그런데 넘어지지 않고 걸어가는 것에 성공한다면 그러면 이것은 좋은 액션이잖아요. 그래서 그 액션에 따라서 저희가 리워드를 주게 됩니다. 넘어지면 마이너스 점수를 넘어지지 않고 잘 걸어가면 플러스 점수를 주는데 그 리워드를 기준으로 저희는 이 액션을 점점 개선해 갈 거예요. 그래서 밑에 설명해 놓은 것처럼 각 스테인마다 저희가 나름대로 어떤 액션을 정합니다. 그러니까 각 스테인마다 이 상황이면 내가 액션을 어떻게 취할 것인가를 먼저 정해 놓는데 그것을 여기 있는 팔로시라고 합니다. 그런데 학습이 잘 되지 않았을 때는 이 팔로시가 성능이 좋지 않겠죠? 그 말은 이 로봇이 자주 넘어질 거예요. 그럼에도 불구하고 팔로시가 하나 정해지면 각 스테인마다 어떤 액션을 취해야 될지가 결정이 됩니다. 그러면 현재 스테이드가 여기 있다고 하겠습니다. 제가 현재 스테이드를 st로 표현하는데 여기서 s는 state의 약자고 t는 time으로 분류하는 건데 현재 스테이드를 제가 t로 표현하겠습니다. 그 말은 현재 노면의 상태와 이 로봇의 관절의 상태 이것이 하나 정해진 거죠. 그런다고 할 때 이 팔로시에 의해서 각 스테인마다 액션이 정해져 있다고 했잖아요? 그러니까 이 스테이드에서 여기 액션이 정해져 있습니다. 이것을 present action이라고 해서 액션의 약자인 a를 따서 at라고 하겠습니다. 이 상황을 다르게 표현한 것이 여기 present state st가 이렇게 있고 이것이 agent에 의해서 인식이 되면 이 agent는 at라는 액션을 취하게 됩니다. 그러면 이 로봇이 이 액션대로 잠깐 움직였으니까 예를 들면 여기 t를 매 0.1초마다 체크한다고 하겠습니다. 그러면 여기 현재 스테이드 st에서 저희가 현재 액션 at를 취한 후에 0.1초가 지난 후에는 다음 스테이드인 st1이 나오겠죠? 그래서 이 액션을 이 환경에 직접 취했더니 그 다음 스테이드로 st1이 나왔습니다. 그리고 이 st1에서도 이 팔로시에 의해서 액션이 하나 정해져 있잖아요? 그 액션을 at1이라고 하면 이 액션을 다시 취하게 되면 어떻게 되는 거죠? 그러면 그 다음 스테이드인 st2가 나오겠죠? 그리고 st2에서도 이 팔로시에 의해서 또 액션을 취하게 되면 그것이 지금 at2인데 그 다음 스테이드인 st3가 나올 거에요. 그래서 이런 방식으로 매 시간에 따라서 스테인마다 액션을 취하게 되고 그 액션을 취해서 그 다음 스테이드를 얻고 그리고 그 다음 스테이드에서 또 액션을 취하고 이게 전부 여기 동일한 팔로시 안에서 하는 거잖아요? 이런 식으로 쭉 하다보면 일정 길이의 데이터셋이 나올 거에요. state action pair에 대한 시퀀스의 데이터셋이 나올 텐데요. 그러면 이제 각 단계마다 저희가 무엇을 할 수 있냐면 이 로봇이 잘 걸어가면 플러스 점수를 중간에 넘어지면 마이너스 점수를 준다고 했잖아요? 그것을 여기는 리워드라고 하는데 여기 각 단계마다 액션을 취한 후에 그 결과로 리워드들이 이렇게 나오겠죠? 그것이 각 타임스템마다 얻게 되는 리워드 rt 플러스 1이 되겠죠? 그래서 강화학습은 기본적으로 현재 스테이지가 주어져 있고 여기에서 이 에이전트가 주어진 팔로시를 기반으로 해서 액션을 취하게 되고 이 액션이 실제 환경에서 작용을 해서 결과로 다음 스테이지가 나오고 이 과정에서 저희는 리워드를 얻습니다. 그런 다음에 이 다음 스테이지를 기반으로 해서 이것을 반복적으로 진행하는 거죠. 그것을 시퀀스로 표현한 것이 밑에 그림이 됩니다. 현재 스테이지에서 팔로시에 의존해서 액션을 취하게 되고 그때 저희가 리워드를 얻게 되고 이제 결과적으로 얻어진 다음 스테이지에서 또 액션을 취하게 되고 여기서 마찬가지로 리워드를 얻게 되는 거죠. 그리고 다음 스테이지로 넘어가는 거죠. 이것을 계속 반복하면서 저희는 데이터를 얻게 됩니다. 그럼 중요한 것은 여기 팔로시 정책이라고 하는데요. 이 정책이 좋은 것이면 여기 있는 리워드 값들이 상당히 좋은 값들이겠죠? 그러면 저희는 이 리워드 값 전체를 sum을 한 여기 Total Reward가 있다면은 이 Total Reward가 클수록 정책이 좋은 거니까 궁극적으로 강화학습에서 트레이닝 할 때는 이 Total Reward를 Maximize 하는 방향으로 이 팔로시를 점점 개선을 합니다. 그래서 강화학습은 크게 두 단계로 나누는데요. 첫 번째는 이 주어진 팔로시에서 이 Total Reward를 계산하는 방법 그것을 팔로시 Evaluation이라고 합니다. 그리고 이렇게 해서 얻어진 Total Reward를 Maximize 하는 쪽으로 팔로시를 개선하는 방법 그것은 팔로시 Improvement라고 합니다. 그래서 강화학습은 이 두 가지 과정을 반복적으로 적용함으로써 점점 팔로시를 개선해갑니다. 그렇기 때문에 강화학습은 여기 Time, 시간이 실제로 아주 중요한 문제가 됩니다. 그리고 또 하나는 이 리워드를 다 얻은 후에 팔로시를 업데이트하기 때문에 그렇기 때문에 피드백이 바로 나오는 것이 아니라 리워드를 얻을 때까지 기다려줘야 되는 거죠. 이것이 강화학습의 특징입니다. 머신러닝의 세 가지 학습방법을 간단히 요약을 해보겠습니다. 여기 머신러닝은 Supervised Learning, Unsupervised Learning, Reinforcement Learning 이렇게 분류가 되는데 먼저 Supervised Learning은 라벨이 있는 데이터를 다룬다고 했습니다. 그래서 인풋으로 데이터가 들어오게 되면 이 네트워크는 outcome으로 그 라벨을 예측해야 되는 거죠. 트레이닝하는 과정에서는 그 라벨을 저희가 알고 있기 때문에 그 예측이 맞았는지 틀렸는지 바로 피드백을 줄 수가 있습니다. 그런데 Unsupervised Learning은 어떻게 되는 거죠? 이것은 라벨이 없는 데이터를 다루기 때문에 바로 피드백을 받을 수가 없습니다. 그래서 이 Unsupervised Learning은 인풋 데이터의 특징을, 패턴을 잘 찾아서 그것의 히든 스트럭처를 찾는 것이 목적이죠. 그런데 이와 달리 Reinforcement Learning은 Decision Process입니다. 이 말은 의사결정 과정이라고 하는데요. 의사가 액션을 어떤 식으로 해야 될지를 결정하는 이런 문제라는 겁니다. 그런데 여기서는 액션을 선택하는 그 기준이 뭐였죠? 여기 리워드를 계산해서 이 리워드를 최대화 시키는 거였잖아요? 그러니까 이것은 리워드 시스템을 가지고 있습니다. 그리고 또 하나는 여기 액션들의 시리즈가 있는데 이 액션들의 시리즈를 학습하는 것, 즉 좋은 판러시를 학습하는 것이 목적이죠. 이런 식으로 해서 크게 세 가지로 분류가 됩니다."
    }
]